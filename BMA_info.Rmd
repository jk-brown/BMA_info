---
title: "BMA_info"
author: "Joe Brown"
date: "2023-03-09"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Bayessian Model Averaging Background

It is often the case that multiple variable predictiona are made from an ensemble of climate models. Bayesian methods can be used to understand models that explain the relationship between predicted variables and observed data ([Hinne et al. 2020](https://journals.sagepub.com/doi/pdf/10.1177/2515245919898657)). Bayesian model selection is the process by which a posterior model probability (PMP) is computed and used to identify the probability of a model given observed data (Hinne et al. 2020). 

**Bayesian model averaging** (BMA) can be used to summarize results when a large number of predictors are being considered. BMA estimates models for all possible combinations of explanatory variables (in our case model results) and constructs a weighted average over each ([BMA package](https://cran.r-project.org/web/packages/BMS/vignettes/bmsmanual.pdf)). A Bayesian model average is defined as a prediction of new observations obtained by averaging the predictions of different candidate models, each of which weighted by its model probability (Hinne et al. 2020). Inclusion probabilities are an example of a summarization method used in BMA (Hinne et al. 2020). 

***In the context of climate models***: BMA has been described as a regression-based approach where the dependent variable is the observed climate variable of interest, and covariates are the outputs of a model ensemble ([Khan et al. 2021](https://link.springer.com/article/10.1007/s10651-021-00490-8)).  

**Posterior inclusion probability** (PIP) can be defined as the model-averaged probability of including a certain predictor in the model given observations. This serves as an indicator of of how relevant a predictor is across all possible models (Hinne et al. 2020). 

***In the context of climate models***: PIP is used as a model selection criterion, where PIP is the sum of the PMPs of each covariate contribution from all possible regression models included in BMA (Kahn et al. 2021). Culka ([2016](https://energsustainsoc.biomedcentral.com/articles/10.1186/s13705-016-0073-0#Sec1)) explains that PIP can be seen as the quality of a covariate in explaining the observed data with respect to all other covariates included in the analysis.
    
The PIP has a range from 0-1, where a value close to 1 means that the corresponding candidate model (i.e., covariate) is a good contributor to explaining the variability in the observed data. Values close to 0 mean the corresponding candidate model does not contribute to explaining the variability (Khan et al. 2021). 

In other non-climate or energy system studies, PIP have offered an intuitive and convenient aid to subset model selection. For example, a subset selection can be made based on PIPs of covariates by setting a threshold value ([Morozova et al. 2015](file:///C:/Users/brow521/Downloads/12874_2015_Article_66.pdf)).

# The [BMS package](https://cran.r-project.org/web/packages/BMS/BMS.pdf)

As stated prior, estimates models for all possible combinations of explanatory variables (in our case model results) and constructs a weighted average over all of them. If a data frame contains K potential covariates (model outputs), this means estimating 2^*K*^ model combinations (i think).  More on defaults are below before running the model. The model selection process using BMA is completed using `bms()`. The following steps shows how to perform BMA with a simple data set and how to obtain posterior coefficients and model stats.  

## Examples:

### Example one

This example uses sample data produced to have 3 models that don't follow the observed data well and one that is exactly identical (mod4).

Start by building a simple data frame. Here, I created a data frame with 4 covariates (which represent separate runs from an `iterate_hector` output). The observed data are stored in the first column of the data frame. The function `bms` assumes that first column is the response variable (observed data). Notice that a random distribution of values are produced for models 1-3, model 4 is identical to the observed data to force a result with a high PIP for this model. 

```{r}
library(BMS)

set.seed(0)
data <- data.frame(
  observed = c(333, 350, 360, 400, 402, 418, 499, 508, 510, 520),
  mod1 = rnorm(10, mean = 550, sd = 100),
  mod2 = rnorm(10, mean = 450, sd = 100),
  mod3 = rnorm(10, mean = 450, sd = 100),
  mod4 = c(333, 350, 360, 400, 402, 418, 499, 508, 510, 520))

head(data) 
```

Performing the model sampling using `bms`. 

```{r}

bmao <- bms(data, 
    mprior = "uniform",
    g = "UIP",
    user.int = F)

```

For the sake of simplicity in this example, I set `mprior = "uniform"` which informs the function to assign a uniform model prior. The unit information prior (`UIP`) is set for Zellners g (`g`) -- still reading about this and its importance as the variance structure of prior coefficients. However, a popular default is to set *g* = *N*. Thus it gives about the same information to the prior as is contained in one observation. It is important to note there are a number of options for setting `g`, there is more information regarding this in the [package vignette](https://cran.r-project.org/web/packages/BMS/vignettes/bmsmanual.pdf)). There is also a summary in this paper by [Amini and Parmeter 2020](https://www.mdpi.com/2225-1146/8/1/6).

### Coefficient Results

```{r}

coef(bmao)

```

This matrix shows variable names and BMA statistics:

 * `Post Mean` - model coefficients averaged over all models. Includes models where the specific model was not contained.
 
 * `PIP` - Posterior inclusion probabilities, the importance of the variable in explaining the data. 
   - *Note: this is the sum of the PMPs for all models where the covariate was included. This can be examined manually in the package by producing the top models, identifying all models with a covariate of interest, producing and taking the sum of the PMPs for those models.*

   - We can see that mod4 is identified as the most important. Virtually all of the posterior model mass relies on models that include mod4, this is also why the PMPs add to 1. Mod3 shows intermediate importance, with other models providing decreasing importance (lower PIPs). The result here is intuitive because mod4 = obs.
   
 * `Post SD` - posterior standard deviations.
 
 * `Cond.Pos.Sign` - "posterior probability of a positive coefficient expected value on inclusion", can be viewed as the certainty of the positive sign of the coefficient.   

_______

### Example 2

This example uses sample data produced to have 2 models that are close to the observed data and 2 that are off. This is a test to compare with Example 1 to see how they differ. 

The code is similar to above with the exception of the data.

```{r}
library(tibble)

# for some reason this does not work...when passing to bms()
data3tib <- tibble(
  observed = c(333, 350, 360, 400, 402, 418, 499, 508, 510, 520),
  mod1 = observed + 2,
  mod2 = observed + 16,
  mod3 = observed - 16,
  mod4 = observed - 4)

data3 <- data.frame(
  observed = c(333, 350, 360, 400, 402, 418, 499, 508, 510, 520),
  mod1 = c(330, 347, 355, 397, 398, 416, 496, 505, 508, 517),
  mod2 = c(223, 240, 250, 390, 392, 408, 488, 498, 550, 610),
  mod3 = c(336, 353, 363, 403, 405, 421, 502, 511, 513, 523),
  mod4 = c(345, 463, 472, 511, 510, 525, 597, 615, 618, 623))

head(data3) 
```

```{r}

bmao2 <- bms(data3,
             mprior = "uniform", user.int = F)

```

```{r}

coeff <- coef(bmao2, condi.coefs = T)

print(coeff)
```
What are the posterior model probabilities:

```{r}

# what are the top models
topmodels.bma(bmao2)

# what are the PMPs for all models
pmp.bma(bmao2, oldstyle = T)

```
We can also pull out PMP values for individual models.

However the PMP value produced is a scalar of the posterior probabilities. 

To use as a weighting value between 0-1, scalar needs to be normalized. This will produce a PMP equivalent to the tradtitional 0-1 values representing PMPs.

```{r}
# this is the PMP scalar for model 3 (top model)
mod3 <- pmpmodel(bmao2, model = "mod3")

sapply(rownames(coeff), function(m) {
  pmpmodel(bmao2, model = m)
})

# this is a df of PMP scalar values from all models
PMPs <- data.frame(pmp.bma(bmao2))

# This is the normalized model 3 PMP
mod3 / sum(PMPs$PMP..Exact.)

```

The following plots show the weighted average of the models when using PIPs as weights vs. PMPs as weights.

```{r}
library(ggplot2)
library(tidyverse)

# adding year column in data
data3$year <- 1990:1999

# convert long format to wide - tidy
PIP_long <- gather(data3, model, value, observed:mod4)

# separate df for observed data
observed <- data.frame(
  year = data3$year,
  value = data3$observed)

# separate df for model predictions - adding wts column (using either PIP or PMP as weights)
projected <- data.frame(
  subset(PIP_long, model != "observed") ,
  PIP_wts = rep(c(0.537, 0.259, 0.539, 0.244), each = 10),
  PMP_wts = rep(c(0.243, 0.026, 0.244,  0.01), each = 10))

# computing a weighted mean column to test how well it matches observed data
wt_projected <- projected %>% 
  group_by(year) %>% 
  mutate(PIP_wt_average = weighted.mean(value, PIP_wts)) %>% 
  mutate(PMP_wt_average = weighted.mean(value, PMP_wts))
```

```{r}
# Try plotting the models against the observed data
observed <- ggplot(data = observed, aes(x = year, y = value)) +
  geom_line(color = "red")

PIP_wt <- observed +
  geom_line(data = projected, aes(x = year, y = value, group = model)) +
  geom_line(data = wt_projected, aes(x = year, y = PIP_wt_average),
            linetype = "dashed",
            color = "blue",
            linewidth = 0.7)
PIP_wt

```

```{r}
PMP_wt <- observed +
  geom_line(data = projected, aes(x = year, y = value, group = model)) +
  geom_line(data = wt_projected, aes(x = year, y = PMP_wt_average),
            linetype = "dashed",
            color = "purple",
            linewidth = 0.7)
PMP_wt

```

Trying to show that using PIP as a model weight over inflates the impact of low ranked models. This because PIP is a sum of all bayes models that include a given covariate. If we want to use a value to represent a weight for how much modeled data should influence the final 

The use of PMPs as weights produces a weighted average of the models included that is much more in line with observed values.

PIPs are still useful in determining how models rank and could be used as cutoff values for skilled vs. unskilled models, but shouldn't be used as weights. 

Using PMPs as weights is consistent with Min and Hense (2006) (and Raferty 2005, although I have not read through this). Min and Hense use bayes factors, which they explain is = PMP when priors are assumed identical, to compute weighted averages for Multi-model ensembles. They show that BMA results of models are closer to observed data than arithmetic ensemble mean.

____

# Example of using MCMC sample 

Markov Chain Monte Carlo sampling is a method used when there are so many variables that enumerating all potential variable combinations is not feasible or too computationally expensive.

More covariates = More time intensive.

MCMC sampling helps with these issues by by gathering results from the most important part of the posterior model distribution. Here, I'm walking through the MCMC sampling solution for a large data set presented in the `BMS` package.

The default `mcmc` method is `bd`. This is the standard model sampler used in most BMA routines.

The quality of the MCMC approximation to the actual posterior distribution depends on the number of draws the MCMC sampler runs through. The first batch of iterations will typically not draw3 models with high PMPs. The sampler will only after a while converge on models with the largest marginal likelihood. the first set of iterations are omitted from the computation of results. In `bms()` this is indicated by the number of `burns`. The argument `iter` indicates the number of subsequent iterations to be retained. 

In this example, the g-prior is set to `BRIC`: g = max(N, K^2^) and bridges g-UIP and g-RIC priors, depending on the values of K. -- Not really sure hat this means yet, still trying to understand g-priors and the differences between each.

The number of top models saved is specified by `nmodel`.

```{r}

data(datafls)

fls1 <- bms(datafls,
            burn = 50000,
            iter = 100000,
            g = "BRIC",
            mprior = "Uniform",
            nmodel = 2000,
            mcmc = "bd",
            user.int = F
            )

summary(fls1)

```

When we view the `summary` information of the bmao we can check the **convergence**. Convergence is the point at which the MCMC algorithm has run long enough to produce samples that are representative of the target posterior distribution. At this point, the samples generated can be used for inference like calculating model probabilities. 

To assess the convergence between MCMC PMPs and analytical PMPs, we can inspect the correlation between MCMC and analytical PMPs by looking at `Corr PMP` in the summary. When this correlation is high, we can be confident that the samples are coming from the intended distribution. The lower this correlation, it is likely there is a lower the degree of convergence. 

```{r}

plotConv(fls1)

```

# Below this line I have not have not thought enough about
____

# Example to see the impact of running 1000 models

```{r, eval=FALSE}
# observed data
observed_data <- rnorm(100, mean = 350, sd = 20)

# dataframe to store results
results <- data.frame(observed_data)

# model labels
model_labels <- paste0("run", 1:100)

# loop through the model iterations
for (i in 1:100) {
  # simulate data for the current model run
  simulated_data <- rnorm(100, mean = 350, sd = 20)
  # add model runs as a new column to the results data frame
  results[[model_labels[i]]] <- simulated_data
}

results <- apply(results, 2, sort)
  
results <- as.data.frame(results[order(results[ ,1]), ])

```

```{r, eval=FALSE}

bmao_large <- bms(results,
             mprior = "uniform", 
             user.int = F,
             g = "UIP",
             burn = 500000,
             iter = 1000000,
             nmodel = 2000,
             mcmc = "bd")

plotConv(bmao_large)

```

```{r, eval=FALSE}

coeff_large <- coef(bmao_large, condi.coefs = T)

```
What are the posterior model probabilities:

```{r, eval=FALSE}

# what are the top models
topmodels <- topmodels.bma(bmao_large)

# what are the PMPs for all models
pmp_vals <- data.frame(pmp.bma(bmao_large, oldstyle = T))

```

Line 285 is an example of a function that will 

```{r, eval=FALSE}
pmp_weights <- data.frame(
  wts = sapply(rownames(coeff_large), function(m) {
  pmpmodel(bmao_large, model = m)
}))

# A function to compute pmp_wts
pmp_wts <- function(coeff_data, bmao){
  
  run_pmps = sapply(rownames(coeff_large), function(m) {
  pmpmodel(bmao_large, model = m)})
  
  pmps = data.frame(
    pmp.bma(bmao, oldstyle = F)[, 1])
  
  rownames(pmps) <- NULL 
  
  wts = data.frame(
    run_wts = run_pmps / sum(pmps))
  
  wts$normalized = wts$run_wts / sum(wts$run_wts)
  
  return(wts)
}

run_wts <- pmp_wts(coeff_large, bmao_large)
```

Code in line 322 and 325 may be important as pieces to combine the weights to `iterate_hector` outputs.
```{r, eval=FALSE}
library(ggplot2)
library(tidyverse)

# adding year column in data
results$year <- 2000:2099

# convert long format to wide - tidy
PIP_long <- gather(results, run, value, run1:run10000)

# separate df for observed data
observed <- data.frame(
  year = results$year,
  value = results$observed_data)

# converting run_wts rowname to column
run_wt <- tibble(rownames_to_column(run_wts, "run"))

# separate df for model predictions - adding wts column (using either PIP or PMP as weights)
projected <- data.frame(
  subset(PIP_long, select = -observed_data))
projected <- merge(projected, run_wt, by = "run")

# computing a weighted mean column to test how well it matches observed data
wt_projected <- projected %>% 
  group_by(year) %>% 
  mutate(wt_average = weighted.mean(value, run_wts))
```

# Using a Hector output 

```{r, eval=FALSE}
# read in example Hector run
hector_data <- read.csv("data/hector_result.csv")

# Function that subsets data and converts to wide
bma_input <- function(df, columns, years, variables){
  split_list <- split(df, df$run_number)
  subset <- lapply(split_list, function(x) subset(x, variables == variable &
                                                  year %in% years,
                                                  select = columns))
  df_long <- do.call(cbind, subset)
  
  return(df_long)
}

test <- bma_input(hector_data, 
          c("value"),
          years = 1959:2021,
          variable = "CO2_concentration")
colnames(test) <- 1:ncol(test)
test$year <- 1959:2021

observed <- read.csv("data/co2_obs.csv")

bma_data <- merge(observed, test, by = "year")[, c(-1, -2)]

```

That is very chaotic but it gets us what we need which is a df of co2 values with observed data in column 1.

```{r, eval=FALSE}
h_bmao <- bms(bma_data,
              mprior = "unifrom",
              user.int = F)

```

What do the coefficients look like

```{r, eval=FALSE}

h_coeff <- coef(h_bmao)

print(h_coeff)
```

What are the PMPs:

```{r, eval =FALSE}

model_PMPs <- pmp.bma(h_bmao, oldstyle = T)

head(model_PMPs)
```

```{r,eval=FALSE}
# this is the PMP scalar for model 3 (top model)
mod3 <- pmpmodel(bmao2, model = "mod3")

pmp_scalar <- data.frame(pmp_scalar = 
                           sapply(rownames(h_coeff), function(m) {
                             pmpmodel(h_bmao, model = m)}))

# this is a df of PMP scalar values from all models
PMPs <- data.frame(pmp.bma(h_bmao))

# This is the normalized model 3 PMP
pmp_scalar$pmp_vals = pmp_scalar$pmp_scalar / sum(PMPs$PMP..Exact.)


```

